{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cleaned Data from raw .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files names - if these files and folders don't exist then they are downloaded \n",
    "raw_file_name = \"quora_duplicate_questions.tsv\"\n",
    "q1_file_name = \"cleaned_q1.txt\"\n",
    "q2_file_name = \"cleaned_q2.txt\"\n",
    "dup_file_name = \"is_duplicate.txt\"\n",
    "questions_folder_name = \"cleaned_data\"\n",
    "\n",
    "# file to store hyperparameter tuning numbers\n",
    "parameters_and_errors_name = \"parameters_and_errors.csv\"# \"parameters_and_errors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file with 404288 rows at quora_duplicate_questions.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "\n",
    "# check whether downloaded data already exists with names provided above\n",
    "if not os.path.isdir(questions_folder_name) or not os.path.isfile(dup_file_name):\n",
    "    \n",
    "    # Download questions file - data set has been changed since first release\n",
    "    url = 'http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "    r = requests.get(url)\n",
    "    with open(raw_file_name, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    # DATA CLEANING WITH PANDAS\n",
    "    \n",
    "    # read in file into dataframe\n",
    "    data = pd.read_csv(raw_file_name, sep='\\t')\n",
    "    # drop rows with null value\n",
    "    data.dropna(inplace=True)\n",
    "    # make columns of lower cased words\n",
    "    data[\"cleaned_q1\"] = data.text1.str.lower()\n",
    "    data[\"cleaned_q2\"] = data.text2.str.lower()\n",
    "    # remove punctuation from lower-cased words columns\n",
    "    data['cleaned_q1'] = data['cleaned_q1'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "    data['cleaned_q2'] = data['cleaned_q2'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "    # remove the character \"\\n\", which messes up the line delimiters in txt file\n",
    "    # these only occur ~20 times in the questions\n",
    "    data[\"cleaned_q1\"] = data['cleaned_q1'].str.replace(\"\\n\", \"\")\n",
    "    data[\"cleaned_q2\"] = data['cleaned_q2'].str.replace(\"\\n\", \"\")\n",
    "    # shuffle data before writing to file - this way random sample can be taken from file \n",
    "    # simply by choosing first n rows of file\n",
    "    data = data.sample(frac=1)\n",
    "\n",
    "    # create directory to hold question data\n",
    "    if not os.path.exists(questions_folder_name):\n",
    "        os.makedirs(questions_folder_name)\n",
    "\n",
    "    # write cleaned text rows to txt files, one line for each sentence\n",
    "    data[\"cleaned_q1\"].to_csv(questions_folder_name + \"/\" + q1_file_name, sep='\\n', header=False, index=False)\n",
    "    data[\"cleaned_q2\"].to_csv(questions_folder_name + \"/\" + q2_file_name, sep='\\n', header=False, index=False)\n",
    "    # write dup values to txt file, one line for each value\n",
    "    data[\"duplicate\"].to_csv(dup_file_name, sep='\\n', header=False, index=False)\n",
    "\n",
    "    print \"Saved file with\", len(data), \"rows at\", raw_file_name\n",
    "else: \n",
    "    print \"Directory that should contain data already exists.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook assumes you have a directory \"questions_directory\" that contains two text files, one for all \n",
    "# first questions and one for all second questions, and a file is_dup_file that has one boolean value (0 or 1)\n",
    "# on each line, one for all ~400,000 document pairs that tells whether questions are duplicates or not\n",
    "\n",
    "# path to questions directory (contains sorted question text files) and \n",
    "# is_dup_file (contains sorted answers about whether question pairs are duplicates)\n",
    "\n",
    "# These files and directories are created automatically by the above cell\n",
    "questions_directory = questions_folder_name\n",
    "is_dup_file = dup_file_name\n",
    "questions_file_names = [os.path.basename(filename) for filename in os.listdir(questions_directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned_q1.txt', 'cleaned_q2.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of rows (question pairs) to train on\n",
    "num_question_pairs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce logs during training\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create iterator to run through directory of text files with one sentence per line\n",
    "from itertools import izip, count\n",
    "\n",
    "# class that iterates through first \"rows\" lines of questions list (\"rows\" is integer)\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, dirname, rows=None):\n",
    "        self.dirname = dirname\n",
    "        self.rows = rows\n",
    "    def __iter__(self):\n",
    "        for filename in os.listdir(self.dirname):\n",
    "            for uid, text_line in enumerate(open(os.path.join(self.dirname, filename))):\n",
    "                if self.rows:\n",
    "                    if uid >= self.rows: \n",
    "                        break\n",
    "            # for uid, line in enumerate(open(os.path.join(self.dirname, filename))):\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=text_line.split(), tags=[os.path.basename(filename) + '_%s' % uid])\n",
    "\n",
    "# make sure using fast version\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "cores = multiprocessing.cpu_count() # number of cores on computer to use for computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 question pairs to train (200 documents total)\n"
     ]
    }
   ],
   "source": [
    "# load in data into memory - all data combined should only be 200-300 megabytes\n",
    "# this is done instead of using iterator - makes doing shuffles of data easier\n",
    "\n",
    "all_docs = []\n",
    "sentences = LabeledLineSentence(questions_directory, rows=num_question_pairs)\n",
    "for sentence in sentences:\n",
    "    all_docs.append(sentence)\n",
    "\n",
    "print('%d question pairs to train (%d documents total)' % (num_question_pairs, len(all_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 document pairs to classify\n",
      "Document pair names and labels contained in doc_names_and_duplicate_class\n"
     ]
    }
   ],
   "source": [
    "# make list of tuples (document1, document2, is_dup) for all num_question_pairs\n",
    "# the document names come from naming scheme used in LabeledLineSentence class \n",
    "\n",
    "doc_names_and_duplicate_class = []\n",
    "for i, line in enumerate(open(is_dup_file)):\n",
    "    if i >= num_question_pairs:\n",
    "        break\n",
    "    doc_tup = (questions_file_names[0] + \"_\" + str(i), questions_file_names[1] + \"_\" + str(i), int(line.strip(\"\\n\")))\n",
    "    doc_names_and_duplicate_class.append(doc_tup)\n",
    "\n",
    "print len(doc_names_and_duplicate_class), \"document pairs to classify\"\n",
    "print \"Document pair names and labels contained in doc_names_and_duplicate_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cleaned_q1.txt_0', 'cleaned_q2.txt_0', 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_names_and_duplicate_class[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_AUC(model, doc_names_and_duplicate_class): \n",
    "    \"\"\" Return area under ROC curve for model. This is done by simply taking cosine similarity between \n",
    "        document vectors to predict whether they are duplicate questions or not.\n",
    "    \"\"\"\n",
    "    doc_distances = []\n",
    "\n",
    "    for i in range(len(doc_names_and_duplicate_class)):\n",
    "        # get word vectors for given pair\n",
    "        vec1_name = doc_names_and_duplicate_class[i][0]\n",
    "        vec2_name = doc_names_and_duplicate_class[i][1]\n",
    "        vec1 = model.docvecs[vec1_name]         \n",
    "        vec2 = model.docvecs[vec2_name]       \n",
    "        # take cosine distance between them\n",
    "        distance = cosine_similarity(vec1, vec2)\n",
    "        doc_distances.append(distance)\n",
    "\n",
    "    doc_distances = np.array(doc_distances)\n",
    "    doc_scores = np.array([x[2] for x in doc_names_and_duplicate_class])\n",
    "    \n",
    "    return roc_auc_score(doc_scores, doc_distances)\n",
    "\n",
    "def cosine_similarity(vec1, vec2): \n",
    "    # return cosine angle between numpy vectors v1 and v2\n",
    "    def unit_vector(vec):\n",
    "        return vec/np.linalg.norm(vec)\n",
    "    vec1_u, vec2_u = unit_vector(vec1), unit_vector(vec2)\n",
    "    return np.dot(vec1_u, vec2_u)\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize Doc2vec model parameters\n",
    "\"\"\"\n",
    "The documents iterable can be simply a list of TaggedDocument elements, \n",
    "but for larger corpora, consider an iterable that streams the documents \n",
    "directly from disk/network.\n",
    "\n",
    "If you don’t supply documents, the model is left uninitialized – \n",
    "use if you plan to initialize it in some other way.\n",
    "\n",
    "\n",
    "\n",
    "dm defines the training algorithm. By default (dm=1), \n",
    "    ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "\n",
    "size is the dimensionality of the feature vectors.\n",
    "\n",
    "window is the maximum distance between the predicted word and context words used for prediction within a document.\n",
    "\n",
    "alpha is the initial learning rate (will linearly drop to zero as training progresses).\n",
    "\n",
    "seed = for the random number generator. Note that for a fully deterministically-reproducible run, \n",
    "    you must also limit the model to a single worker thread, to eliminate ordering jitter from \n",
    "    OS thread scheduling. (In Python 3, reproducibility between interpreter launches also \n",
    "    requires use of the PYTHONHASHSEED environment variable to control hash randomization.)\n",
    "\n",
    "min_count = ignore all words with total frequency lower than this.\n",
    "\n",
    "max_vocab_size = limit RAM during vocabulary building; if there are more unique words \n",
    "                 than this, then prune the infrequent ones. Every 10 million word types \n",
    "                 need about 1GB of RAM. Set to None for no limit (default).\n",
    "\n",
    "sample = threshold for configuring which higher-frequency words are randomly downsampled;\n",
    "        default is 0 (off), useful value is 1e-5.\n",
    "        \n",
    "workers = use this many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "iter = number of iterations (epochs) over the corpus. The default inherited from Word2Vec is 5, \n",
    "        but values of 10 or 20 are common in published ‘Paragraph Vector’ experiments.\n",
    "\n",
    "hs = if 1 (default), hierarchical sampling will be used for model training (else set to 0).\n",
    "\n",
    "negative = if > 0, negative sampling will be used, the int for negative specifies how many \n",
    "            “noise words” should be drawn (usually between 5-20).\n",
    "\n",
    "dm_mean = if 0 (default), use the sum of the context word vectors. If 1, use the mean. \n",
    "            Only applies when dm is used in non-concatenative mode.\n",
    "\n",
    "dm_concat = if 1, use concatenation of context vectors rather than sum/average; default is 0 (off). \n",
    "                Note concatenation results in a much-larger model, as the input is no longer the size of \n",
    "                one (sampled or arithmatically combined) word vector, but the size of the tag(s) and all \n",
    "                words in the context strung together.\n",
    "\n",
    "dm_tag_count = expected constant number of document tags per document, when using dm_concat mode; default is 1.\n",
    "\n",
    "dbow_words if set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW doc-vector training; \n",
    "            default is 0 (faster training of doc-vectors only).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# set model parameters parameters\n",
    "\n",
    "parameters_dict = {\n",
    "\n",
    "'documents' : all_docs,\n",
    "'dm' : 0, # use bag-of-words (dbow) model; 1 uses embedding (dmpv) model\n",
    "'size' : 200, # size of word/doc vectors\n",
    "'window' : 15, # # max distance between word and neighbor word for word embeddings\n",
    "'alpha' : .025, # learning rate - use rate in paper\n",
    "'min_alpha' : 0.0001, # rate from paper\n",
    "'min_count' : 5, # ignore words with count less than this\n",
    "'sample' : 1e-5, # how to configure downsampling for high frequency words\n",
    "'workers' : cores, # number of cores to use\n",
    "'hs' : 0, # use negative sampling\n",
    "'negative' : 5, # used in negative sampling\n",
    "'dbow_words' : 1, # trains word vectors in addition to document vectors in dbow model\n",
    "'iter' : 3 # recommended number of epochs is ~20 for dbow model on question comparison   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parameters for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create list of parameters to use in model\n",
    "dms = [0]\n",
    "sizes = [300]\n",
    "windows = [5, 15]\n",
    "alphas = [0.025]\n",
    "min_alphas = [0.0001]\n",
    "min_counts = [1, 5]\n",
    "samples = [1e-5, 5e-5, 1e-4]\n",
    "workers_s = [cores]\n",
    "hs_s = [0]\n",
    "negatives = [5]\n",
    "dbow_words_s = [1]\n",
    "iters = [150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run through all parameters and record error rate of each one\n",
    "from itertools import product\n",
    "\n",
    "# create list to score ROC AUC scores and their model parameters\n",
    "params_and_errors = []\n",
    "# create iterable of all combinations of parameters\n",
    "params_product = product(dms, sizes, windows, alphas, min_alphas, \n",
    "                        min_counts, samples, workers_s, hs_s, negatives, \n",
    "                        dbow_words_s, iters)\n",
    "parameters = [x for x in params_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting first run of 12 runs\n",
      "\n",
      "Completed run number 1 of 12 runs total\n",
      "AUC score: 0.3437\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 2 of 12 runs total\n",
      "AUC score: 0.6061\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 3 of 12 runs total\n",
      "AUC score: 0.7381\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 4 of 12 runs total\n",
      "AUC score: 0.4251\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 5 of 12 runs total\n",
      "AUC score: 0.4496\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 6 of 12 runs total\n",
      "AUC score: 0.479\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 7 of 12 runs total\n",
      "AUC score: 0.3148\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 8 of 12 runs total\n",
      "AUC score: 0.6298\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 9 of 12 runs total\n",
      "AUC score: 0.7259\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 10 of 12 runs total\n",
      "AUC score: 0.4274\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 11 of 12 runs total\n",
      "AUC score: 0.4414\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "Completed run number 12 of 12 runs total\n",
      "AUC score: 0.493\n",
      "Training for this run took 0.0 minutes\n",
      "\n",
      "\n",
      "Total training time for all runs: 0.0 hours\n",
      "Best AUC value: 0.763003\n",
      "Paramters for best AUC value: {'dm': 0, 'hs': 0, 'sample': 0.0001, 'dbow_words': 1, 'alpha': 0.025, 'min_count': 1, 'size': 300, 'workers': 8, 'negative': 5, 'iter': 150, 'window': 5, 'min_alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print \"Starting first run of\", len(parameters), \"runs\"\n",
    "total_time = 0\n",
    "for run_number, pars in enumerate(parameters): \n",
    "    params = {'dm':pars[0], 'size':pars[1], 'window':pars[2], \n",
    "              'alpha':pars[3], 'min_alpha':pars[4], 'min_count':pars[5],\n",
    "              'sample':pars[6], 'workers':pars[7], 'hs':pars[8],\n",
    "              'negative':pars[9], 'dbow_words':pars[10], 'iter':pars[11]}\n",
    "    with elapsed_timer() as elapsed:\n",
    "        model = gensim.models.doc2vec.Doc2Vec(documents=all_docs, **params)\n",
    "        AUC_value = calculate_AUC(model, doc_names_and_duplicate_class)\n",
    "        params_and_errors.append((params, AUC_value))\n",
    "        duration = '%.1f' % elapsed()\n",
    "        total_time += float(duration)\n",
    "        print \n",
    "        print \"Completed run number\", run_number + 1, \"of\", len(parameters), \"runs total\"\n",
    "        print \"AUC score:\", round(AUC_value, 4)\n",
    "        print \"Training for this run took\", round(float(duration)/60.,1), \"minutes\"\n",
    "\n",
    "best_AUC = max([x[1] for x in params_and_errors])\n",
    "print\n",
    "print\n",
    "print \"Total training time for all runs:\", round(float(total_time)/3600.,2), \"hours\"\n",
    "print \"Best AUC value:\", round(best_AUC, 6)\n",
    "print \"Paramters for best AUC value:\", [x[0] for x in params_and_errors if x[1] == best_AUC][0]\n",
    "\n",
    "# convert params and errors into easy-to-read pandas dataframe\n",
    "params_df = pd.DataFrame([x[0] for x in params_and_errors])\n",
    "params_df[\"AUC\"] = pd.Series([x[1] for x in params_and_errors])\n",
    "params_df[\"num_doc_pairs\"] = pd.Series([len(all_docs) for _ in range(len(params_and_errors))])\n",
    "params_df.sort_values(\"AUC\", ascending=False, inplace=True)\n",
    "\n",
    "# write parameter values to csv - append if this csv already exists\n",
    "header=True\n",
    "if os.path.isfile(parameters_and_errors_name):\n",
    "    header=False\n",
    "params_df.to_csv(parameters_and_errors_name, header=header, index=False, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>dbow_words</th>\n",
       "      <th>dm</th>\n",
       "      <th>hs</th>\n",
       "      <th>iter</th>\n",
       "      <th>min_alpha</th>\n",
       "      <th>min_count</th>\n",
       "      <th>negative</th>\n",
       "      <th>sample</th>\n",
       "      <th>size</th>\n",
       "      <th>window</th>\n",
       "      <th>workers</th>\n",
       "      <th>AUC</th>\n",
       "      <th>num_doc_pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.763003</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.741746</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.738128</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.730891</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.725916</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.715061</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.634555</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.629806</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.596789</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.545907</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.495251</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.492990</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.484396</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.478969</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.475351</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.473089</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.464948</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.456807</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.449570</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.443238</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.441429</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.433288</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.427408</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.425147</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.418363</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.416101</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.360470</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.347806</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.343736</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.326549</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.326097</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.314790</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  dbow_words  dm  hs  iter  min_alpha  min_count  negative   sample  \\\n",
       "14  0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "2   0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "26  0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "20  0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "32  0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "8   0.025           1   0   0   150     0.0001          1         5  0.00010   \n",
       "7   0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "31  0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "19  0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "25  0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "1   0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "13  0.025           1   0   0   150     0.0001          1         5  0.00005   \n",
       "11  0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "35  0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "17  0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "29  0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "5   0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "10  0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "16  0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "23  0.025           1   0   0   150     0.0001          5         5  0.00010   \n",
       "28  0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "22  0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "34  0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "4   0.025           1   0   0   150     0.0001          5         5  0.00005   \n",
       "3   0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "33  0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "27  0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "15  0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "21  0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "9   0.025           1   0   0   150     0.0001          5         5  0.00001   \n",
       "6   0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "0   0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "24  0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "12  0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "18  0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "30  0.025           1   0   0   150     0.0001          1         5  0.00001   \n",
       "\n",
       "    size  window  workers       AUC  num_doc_pairs  \n",
       "14   300       5        8  0.763003            200  \n",
       "2    300       5        8  0.741746            200  \n",
       "26   300       5        8  0.738128            200  \n",
       "20   300      15        8  0.730891            200  \n",
       "32   300      15        8  0.725916            200  \n",
       "8    300      15        8  0.715061            200  \n",
       "7    300      15        8  0.634555            200  \n",
       "31   300      15        8  0.629806            200  \n",
       "19   300      15        8  0.626866            200  \n",
       "25   300       5        8  0.606061            200  \n",
       "1    300       5        8  0.596789            200  \n",
       "13   300       5        8  0.545907            200  \n",
       "11   300      15        8  0.495251            200  \n",
       "35   300      15        8  0.492990            200  \n",
       "17   300       5        8  0.484396            200  \n",
       "29   300       5        8  0.478969            200  \n",
       "5    300       5        8  0.475351            200  \n",
       "10   300      15        8  0.473089            200  \n",
       "16   300       5        8  0.464948            200  \n",
       "23   300      15        8  0.456807            200  \n",
       "28   300       5        8  0.449570            200  \n",
       "22   300      15        8  0.443238            200  \n",
       "34   300      15        8  0.441429            200  \n",
       "4    300       5        8  0.433288            200  \n",
       "3    300       5        8  0.432836            200  \n",
       "33   300      15        8  0.427408            200  \n",
       "27   300       5        8  0.425147            200  \n",
       "15   300       5        8  0.424242            200  \n",
       "21   300      15        8  0.418363            200  \n",
       "9    300      15        8  0.416101            200  \n",
       "6    300      15        8  0.360470            200  \n",
       "0    300       5        8  0.347806            200  \n",
       "24   300       5        8  0.343736            200  \n",
       "12   300       5        8  0.326549            200  \n",
       "18   300      15        8  0.326097            200  \n",
       "30   300      15        8  0.314790            200  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show only the columns that changed in value \n",
    "params_df.loc[:, (params_df != params_df.ix[0]).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" PARAMETER TIPS: \n",
    "\n",
    "sample: CRITICAL: 1e-6 terrible, 5e-6 and 1e-5 better - try 5e-5 \n",
    "min_count: 1 works well with small data set\n",
    "window: 5 and 15 both worked reasonably well\n",
    "size: 300 works well\n",
    "iter: 150 seems ok, try more - sentence similarity in paper used ~400 to converge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in paramater file to see what has been done before\n",
    "all_params_df = pd.read_csv(parameters_and_errors_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.672 with 100-length vectors and 20 epochs\n",
    "# 0.671 with 300-length vectors and 20 epochs\n",
    "# 0.74 with 200-length vectors and 200 epochs\n",
    "# 0.74 with 200-length vectors and 100 epochs\n",
    "\n",
    "# model seems to tail off in accuracy around 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual runs with accuracy check at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try a run with manual epochs and recording accuracy after each epoch\n",
    "model = gensim.models.doc2vec.Doc2Vec(documents=None, dm=dm, size=size, window=window, \n",
    "                                      alpha=alpha, min_alpha=min_alpha, min_count=min_count, \n",
    "                                      sample=sample, workers=workers, hs=hs, negative=negative, \n",
    "                                      dbow_words=dbow_words, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# number of epochs to train\n",
    "# need to manually decrease alpha in this case\n",
    "epochs = 100\n",
    "alpha = 0.025\n",
    "min_alpha = 0.0001\n",
    "alpha_delta = (alpha - min_alpha) / epochs\n",
    "\n",
    "model.build_vocab(all_docs)\n",
    "with elapsed_timer() as elapsed:\n",
    "    for epoch in range(epochs): \n",
    "        model.alpha, model.min_alpha = alpha, alpha\n",
    "        # shuffle documents\n",
    "        shuffle(all_docs)\n",
    "        # train model\n",
    "        model.train(all_docs)\n",
    "        # evaluate model \n",
    "        error = calculate_AUC(model, doc_names_and_duplicate_class)\n",
    "        alpha -= alpha_delta\n",
    "        print \"AUC for epoch\", epoch, \":\", error\n",
    "    duration = '%.1f' % elapsed()\n",
    "    print(\"completed training in %s minutes\" % (duration/60.))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
