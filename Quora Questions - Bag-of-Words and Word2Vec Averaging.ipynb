{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Duplicate Questions Classification Task\n",
    "\n",
    "Quora is a question/answer website where users can ask and answer questions about anything. To improve the website for users, Quora wants to remove duplicate questions, so users are always directed to the one definitive page for that question. Zero percent duplication is unlikely, but automated \n",
    "\n",
    "To help with this goal, Quora has released a data set of ~400,000 question pairs from Quora, where each pair is marked as being either a duplicate or a non-duplicate. Release statement and data found here: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n",
    "\n",
    "For example, \n",
    "\n",
    "#### \"How do you start a bakery?\" and \"How does one start a bakery business?\" \n",
    "\n",
    "are labelled as duplicates, but \n",
    "\n",
    "#### \"Does society place too much importance on sports?\" and \"How do sports contribute to the society?\"\n",
    "\n",
    "are not. The labeling itself was done by humans, so reasonable people may disagree about some of them - such label noise is a fact of life for many classification tasks. \n",
    "\n",
    "The classification goal is to predict whether each question pair is labelled as duplicates or not. \n",
    "\n",
    "It is important to consider how the model will be used to know what metrics to optimize. For example, it is not necessarily true that we want a model that is as accurate as possible: if the model will lead to automatic deletion of one of the posts marked \"duplicate\", it may be more important to avoid falsely labeling different posts as duplicates (false positives) than to miss marking true duplicates as duplicates (false negatives). These considerations are why precision the precision and recall of a classification model are important to consider in addition to its accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "It's always a good idea to check data set for missing values, unusual values, unusual data types, etc. Since this is a prepared data set released by a company it should be pretty well-formed already, but it's good practice to check anyway. It's also good to constantly be looking at examples of data to understand better what your model must accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(data.question1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105796</th>\n",
       "      <td>105796</td>\n",
       "      <td>209841</td>\n",
       "      <td>209842</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201871</th>\n",
       "      <td>201871</td>\n",
       "      <td>398348</td>\n",
       "      <td>398349</td>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                         question1 question2  \\\n",
       "105796  105796  209841  209842  How can I develop android app?    NaN        \n",
       "201871  201871  398348  398349  How can I create an Android app?  NaN        \n",
       "\n",
       "        is_duplicate  \n",
       "105796  0             \n",
       "201871  0             "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find if any rows have null values \n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete rows that contain null values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful command that lets us see full string in columns; otherwise string is truncated \n",
    "# as in data.head() above\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of actual question comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[[\"question1\", \"question2\", \"is_duplicate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at data I notice a few things: \n",
    "\n",
    "#### 1) There can be reasonable disagreement about whether questions are duplicates. \n",
    "\n",
    "For example, at index 19, \"Why do rockets look white?\" could be considered slightly different from \"Why are rockets and boosters painted white\" - in fact, the answer to the first question might be \"because they are painted white\" (as opposed to something like \"atmospheric effects make them look white during liftoff\"), which might naturally lead to the second question. In thise dataset though, these questions are considered duplicates. This type of \"reasonable disagreement noise\" in labeling is common, unavoidable, and important to be aware of.\n",
    "\n",
    "#### 2) For questions that are not considered duplicates, there appear to be two classes: ones where the two questions are closely related but distinct, and ones where the two questions are totally unrelated.\n",
    "\n",
    "For example, at index 0 the questions \"What is the step by step guide to invest in share market in India?\" and \"What is the step by step guide to invest in share market?\" are closely related but logically distinct, as one has a narrower focus than the other. However, at index 3 we have \n",
    "\n",
    "\"Why am I mentally very lonely? How can I solve it?\" \n",
    "\n",
    "being compared with \n",
    "\n",
    "\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
    "\n",
    "which are completely unrelated. This is likely due to the sampling issue mentioned in data release: \n",
    "\n",
    "\"Our original sampling method returned an imbalanced dataset with many more true examples of duplicate pairs than non-duplicates. Therefore, we supplemented the dataset with negative examples. One source of negative examples were pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent.\"\n",
    "\n",
    "This is useful to know for model creation, as we now know we cannot assume the two questions will be somewhat related - they could be completely different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "Always a good idea to try an extremely simple model first. This helps set a baseline for what is considered a \"good\" model.\n",
    "\n",
    "For this data set, the simplest conceivable model would be to predict whichever class is most common in the data.\n",
    "\n",
    "This model predicts every item as \"not duplicate\" and gets 63% accuracy. It also has virtue of not falsely predicting any true different posts as duplicates (i.e., no false positives), which is probably a particularly important characteristic for a model used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"Number of 0 labels:\", len(data[data.is_duplicate == 0])\n",
    "print \"Number of 1 labels:\", len(data[data.is_duplicate == 1])\n",
    "print \"Accuracy to predict most common label (in this case 0):\", len(data[data.is_duplicate == 0])/float(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features\n",
    "\n",
    "For text data, a common feature set is weighted term frequenices. Each text document is then represented as a tf-idf vector, which uses word frequencies between documents (normalized by how often the words appear across documents) as features. \n",
    "\n",
    "The sklearn implementation provides options to remove \"stop words\" (common english words like \"the\" that are unlikely to communicate information content), remove words that appear too frequently or too rarely, and to select how many words total should be considered features when building a feature vector. It also has ability to extract N-grams, which help preserve some of ordering information that a regular frequency count completely loses.\n",
    "\n",
    "Because the classification task involves comparing two documents (in this case two questions), we will need a way to compare the two feature vectors. The standard way to do this for documents is to compute the dot product between their tf-idf vectors, called the cosine similarity. The sklearn tf-idf vectors are normalized to unit length already, so we can use a regular dot product without dividing by vectors lengths.  \n",
    "\n",
    "### Feature ideas: \n",
    "\n",
    "0) explore distribution of dot products between questions for each feature idea\n",
    "\n",
    "0.5) use hashing trick for feature vector creation to increase speed if don't need to convert vectors back to words\n",
    "\n",
    "1) cross-validate to test parameters for max_df, min_df, and max_features\n",
    "\n",
    "2) binary occurence model (instead of tf-idf - supposed to be better for small documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, max_df=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataframe with one column for all questions so they shared same vocabulary in \n",
    "# word frequency model\n",
    "all_questions = pd.concat([data[\"question1\"], data[\"question2\"]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorize all questions together for one common vocabulary and feature space\n",
    "all_Q = vectorizer.fit_transform(all_questions.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_Q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = all_Q[0:all_Q.shape[0]/2]\n",
    "Q2 = all_Q[all_Q.shape[0]/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Q1 shape:\", Q1.shape\n",
    "print \"Q2 shape:\", Q2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows how we can use the tf-idf vector to go back to word representation (though with order information lost). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# gives list of features in alphabetical order\n",
    "word_bank = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in Q1[0,:].nonzero()[1]:\n",
    "    print word_bank[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in Q2[0,:].nonzero()[1]:\n",
    "    print word_bank[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next show how to take a dot product between two sparse vectors. \n",
    "\n",
    "The .T takes transpose and .A converts back to a regular numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = Q1[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.dot(row, row.T).A[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try dotting first question from Q1 and Q2 together - very nearly 1, showing related question, \n",
    "# tho they are not considered duplicates\n",
    "np.dot(Q1[0,:], Q2[0,:].T).A[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now try dotting two unrelated questions together - zero, as expected \n",
    "np.dot(Q1[0,:], Q2[4,:].T).A[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lastly, try dot between two questions (index 5) that are considered duplicates: \n",
    "# only 0.48, which suggests a model based only on term counts or term frequencies will have \n",
    "# trouble achieving high accuracy\n",
    "np.dot(Q1[5,:], Q2[5,:].T).A[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tf-idf vectors products to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make columns of dots between Q1 and Q2\n",
    "tf_idf_dot_column = [np.dot(Q1[i,:], Q2[i,:].T).A[0,0] for i in range(Q1.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_dot_products = pd.Series(np.array(tf_idf_dot_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tf_idf_dot_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.assign(tf_idf_dot_products=tf_idf_dot_products.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot tf_idf dot products for duplicates and non-duplicates\n",
    "\n",
    "We see from charts below that non-duplicates have a spike at 0 and duplicates have a spike at 1, as expected. There appears to be a vast overlap in the middle (meaning in the area where questions use some of the same words), which will be the hardest pairs to correctly classify under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dot product distribution for non-duplicates\n",
    "data[data.is_duplicate == 0].tf_idf_dot_products.plot(\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dot product distribution for duplicates\n",
    "data[data.is_duplicate == 1].tf_idf_dot_products.plot(\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# histograms make this more clear: non-duplicates have a big spike at zero overlap\n",
    "data[data.is_duplicate == 0].tf_idf_dot_products.plot(\"hist\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# true duplicates have a big spike at near-total overlap\n",
    "data[data.is_duplicate == 1].tf_idf_dot_products.plot(\"hist\", bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make simple with tf-idf dot products, show ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.tf_idf_dot_products.values\n",
    "y = data.is_duplicate.values\n",
    "X = X.reshape(-1,1) # numpy will require features to have this shape in future \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, X_test)\n",
    "plt.plot(fpr, tpr) \n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\") \n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\") \n",
    "plt.title(\"ROC plot of Duplicate Questions Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute auc score\n",
    "roc_auc_score(y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different numbers of n-grams with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_2gram = TfidfVectorizer(stop_words='english', use_idf=True, max_df=0.5, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataframe with one column for all questions so they shared same vocabulary in \n",
    "# word frequency model\n",
    "all_questions = pd.concat([data[\"question1\"], data[\"question2\"]], ignore_index=True)\n",
    "# vectorize all questions together for one common vocabulary and feature space\n",
    "all_Q = vectorizer_2gram.fit_transform(all_questions.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = all_Q[0:all_Q.shape[0]/2]\n",
    "Q2 = all_Q[all_Q.shape[0]/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make columns of dots between Q1 and Q2 - this takes about 15 minutes to run\n",
    "tf_idf_2gram_dot_column = [np.dot(Q1[i,:], Q2[i,:].T).A[0,0] for i in range(Q1.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_2gram_dot_products = pd.Series(np.array(tf_idf_2gram_dot_column))\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data.assign(tf_idf_2gram_dot_products=tf_idf_2gram_dot_products.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot ROC curve for 2-gram dot products\n",
    "X = data.tf_idf_2gram_dot_products.values\n",
    "y = data.is_duplicate.values\n",
    "X = X.reshape(-1,1) # numpy will require features to have this shape in future \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, X)\n",
    "plt.plot(fpr, tpr) \n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\") \n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\") \n",
    "plt.title(\"ROC plot of Duplicate Questions Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute auc score for 2gram model - it's worse than 1-gram!\n",
    "roc_auc_score(y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Embeddings with Gensim\n",
    "\n",
    "Here we'll need to do some preprocessing on words - we'll lowercase everything and remove punctuation, and make text file that contains a different question on each line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write cleaned questions to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim # this version is able to use pre-loaded word embeddings for doc2vec\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lowercase words, remove punctuation and print to text file to prepare for learning\n",
    "# (sklearn did that automatically with the vectorizer function for tf-idf)\n",
    "data[\"cleaned_q1\"] = data.question1.str.lower()\n",
    "data[\"cleaned_q2\"] = data.question2.str.lower()\n",
    "\n",
    "# remove punctuation\n",
    "data['cleaned_q1'] = data['cleaned_q1'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "data['cleaned_q2'] = data['cleaned_q2'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the character \"\\n\", which messed up the line delimiters in txt file\n",
    "# these only occur a few times in the questions\n",
    "data[\"cleaned_q1\"] = data['cleaned_q1'].str.replace(\"\\n\", \"\")\n",
    "data[\"cleaned_q2\"] = data['cleaned_q2'].str.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"cleaned_q1\"].to_csv(\"cleaned_q1.txt\", sep='\\n', header=False, index=False)\n",
    "data[\"cleaned_q2\"].to_csv(\"cleaned_q2.txt\", sep='\\n', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write file giving true values for whether questions are duplicates or not\n",
    "data[\"is_duplicate\"].to_csv(\"is_duplicate.txt\", sep='\\n', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[0:10][\"cleaned_q1\"].to_csv(\"test_q1.txt\", sep='\\n', header=False, index=False)\n",
    "data[0:10][\"cleaned_q2\"].to_csv(\"test_q2.txt\", sep='\\n', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0:10][\"is_duplicate\"].to_csv(\"test_is_duplicate.txt\", sep='\\n', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sample of 20% of data to use for tuning hyperparameters\n",
    "data_sample = data.sample(frac=.20, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sample[\"cleaned_q1\"].to_csv(\"20percent_q1.txt\", sep='\\n', header=False, index=False)\n",
    "data_sample[\"cleaned_q2\"].to_csv(\"20percent_q2.txt\", sep='\\n', header=False, index=False)\n",
    "data_sample[\"is_duplicate\"].to_csv(\"20percent_is_dup.txt\", sep='\\n', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try training on questions themselves as corpus to generate word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create iterator to iterate over questions in memory-friendly way, as per word2vec gensim tutorial here: \n",
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "import os\n",
    "\n",
    "# iterates over all files ina directory\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "# class to iterate through single file \n",
    "class OneFileSentences(object): \n",
    "    def __init__(self, filename): \n",
    "        self.filename = filename\n",
    "    \n",
    "    def __iter__(self): \n",
    "        for line in open(self.filename): \n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sentences iterator and train model\n",
    "sentences = MySentences('cleaned_questions')\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create function to turn each sentence into average of its constituent word vectors\n",
    "\n",
    "def make_question_vectors(model, sentence): \n",
    "    # return numpy document vector by averaging constituent word vectors\n",
    "    # sentence is a list of words in same style as iterator makes for entering into word2vec\n",
    "    word_vecs = []\n",
    "    for word in sentence: \n",
    "        try: \n",
    "            new_word = model[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # check whether array has nan before appending\n",
    "        if not np.isnan(np.sum(new_word)):\n",
    "            word_vecs.append(new_word)\n",
    "    # if no appropriate word vectors found, return array of zeros\n",
    "    if not word_vecs:\n",
    "        return np.zeros(model.layer1_size)\n",
    "    word_vecs = np.array(word_vecs)\n",
    "    return word_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make numpy list of normalized dot products of word vectors averages between corresponding questions \n",
    "import itertools\n",
    "\n",
    "q1_sentences = OneFileSentences('cleaned_questions/cleaned_q1.txt')\n",
    "q2_sentences = OneFileSentences('cleaned_questions/cleaned_q2.txt')\n",
    "\n",
    "document_distances = []\n",
    "\n",
    "# for each sentence pair, dot the numpy vectors together to find their distance\n",
    "for q1, q2 in itertools.izip(q1_sentences, q2_sentences): \n",
    "    q1_vec, q2_vec = make_question_vectors(model, q1), make_question_vectors(model, q2)\n",
    "    # take normed dot product, accounting for zero vectors\n",
    "    dot_prod = np.dot(q1_vec, q2_vec)\n",
    "    norm = np.linalg.norm(q1_vec)*np.linalg.norm(q2_vec)\n",
    "    norm_dot_prod = 0\n",
    "    if dot_prod != 0: \n",
    "        norm_dot_prod = dot_prod/float(norm)\n",
    "    document_distances.append(norm_dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find AUC score for this classification threshold\n",
    "doc_dist = np.array(document_distances)\n",
    "y = data.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, doc_dist)\n",
    "plt.plot(fpr, tpr) \n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\") \n",
    "plt.ylabyel(\"True Positive Rate (Sensitivity, Recall)\") \n",
    "plt.title(\"ROC plot of Duplicate Questions Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute auc score for average word2vec model - it's worse than tf-idf!\n",
    "roc_auc_score(y, doc_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next try averaging using pre-trained Google News Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this model takes about 2 minutes to load\n",
    "import gensim\n",
    "model_path = '/Users/danieldandurand/Desktop/projects/word_embeddings/doc2vec_fork/word2vec_binaries/GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this takes about 5 minutes to calculate\n",
    "# make numpy list of normalized dot products of word vectors averages between corresponding questions \n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "q1_sentences = OneFileSentences('cleaned_questions/cleaned_q1.txt')\n",
    "q2_sentences = OneFileSentences('cleaned_questions/cleaned_q2.txt')\n",
    "\n",
    "document_distances = []\n",
    "\n",
    "# for each sentence pair, dot the numpy vectors together to find their distance\n",
    "for q1, q2 in itertools.izip(q1_sentences, q2_sentences): \n",
    "    q1_vec, q2_vec = make_question_vectors(model, q1), make_question_vectors(model, q2)\n",
    "    # take normed dot product, accounting for zero vectors\n",
    "    dot_prod = np.dot(q1_vec, q2_vec)\n",
    "    norm = np.linalg.norm(q1_vec)*np.linalg.norm(q2_vec)\n",
    "    norm_dot_prod = 0\n",
    "    if dot_prod != 0: \n",
    "        norm_dot_prod = dot_prod/float(norm)\n",
    "    document_distances.append(norm_dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find AUC score for this classification threshold\n",
    "gnews_dist = np.array(document_distances)\n",
    "y = data.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x202133e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x252b17b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, gnews_dist)\n",
    "plt.plot(fpr, tpr) \n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\") \n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\") \n",
    "plt.title(\"ROC plot of Duplicate Questions Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72589993258326513"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute auc score for word2vec average using Googlenews model\n",
    "roc_auc_score(y, gnews_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
